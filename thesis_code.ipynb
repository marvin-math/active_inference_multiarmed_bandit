{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marvin-math/active_inference_multiarmed_bandit/blob/main/thesis_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVW7ox8UeAH_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from statistics import mean\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "#from scipy.special import psi\n",
        "\n",
        "# create Bandit class\n",
        "class BayesianBanditAgent:\n",
        "    def __init__(self, num_arms, prior_mean, prior_precision, prior_alpha, prior_beta, eta, decay, ucb_weight):\n",
        "        self.num_arms = num_arms\n",
        "        self.prior_mean = prior_mean\n",
        "        self.prior_precision = prior_precision\n",
        "        self.prior_alpha = prior_alpha\n",
        "        self.prior_beta = prior_beta\n",
        "        self.draws = np.zeros(num_arms)\n",
        "        self.posterior_dist_mean = np.ones(num_arms) # this has to be reworked\n",
        "        self.posterior_variance = np.ones(num_arms)\n",
        "        self.posterior_mean = np.ones(num_arms)\n",
        "        self.posterior_precision = np.ones(num_arms)\n",
        "        self.posterior_alpha = np.ones(num_arms)\n",
        "        self.posterior_beta = np.ones(num_arms)\n",
        "        self.rewards = np.zeros(num_arms)\n",
        "        self.observations = np.empty(num_arms, dtype=object)\n",
        "        self.observations[:] = [[] for _ in range(num_arms)]\n",
        "        self.obs_mean = np.zeros(num_arms)\n",
        "        self.t_dof = np.zeros(num_arms)\n",
        "        self.t_mean = np.zeros(num_arms)\n",
        "        self.t_precision = np.zeros(num_arms)\n",
        "        self.t_sd = np.zeros(num_arms)\n",
        "        self.predictive_pdf = np.ones(num_arms)\n",
        "        self.entropy_predictive_pdf = np.ones(num_arms)\n",
        "        self.eta = eta\n",
        "        self.kl_div = np.ones(num_arms)\n",
        "        self.ambiguity_term = np.ones(num_arms)\n",
        "        self.expected_free_energy = np.ones(num_arms)\n",
        "        self.expected_free_energies = np.empty(num_arms, dtype=object)\n",
        "        self.expected_free_energies[:] = [[] for _ in range(num_arms)]\n",
        "        self.decay = decay\n",
        "        self.weight_current = np.ones(num_arms)\n",
        "        self.moving_avg_previous = np.zeros(num_arms)\n",
        "        self.weight_previous = np.zeros(num_arms)\n",
        "        self.moving_avg_current = np.zeros(num_arms)\n",
        "        self.thompson_mean = np.ones(num_arms)\n",
        "        self.ucb_weight = ucb_weight\n",
        "        self.ucb = np.ones(num_arms)\n",
        "        self.action_values = np.ones(num_arms)\n",
        "        self.current_reward = np.ones(num_arms)\n",
        "        self.softmax_probs = np.zeros(num_arms)\n",
        "\n",
        "\n",
        "\n",
        "    def select_lowest_expected_free_energy_arm(self):\n",
        "        min_expected_free_energy = float('inf')\n",
        "        best_arm = 0\n",
        "\n",
        "        for arm in range(self.num_arms):\n",
        "            self.expected_free_energy[arm] = self.calc_expected_free_energy(arm)\n",
        "            self.expected_free_energies[arm].append(self.expected_free_energy[arm])\n",
        "            #if self.expected_free_energy[arm] <= min_expected_free_energy: # a bit ambiguity here, but probably not relevant\n",
        "                #min_expected_free_energy = self.expected_free_energy[arm]\n",
        "                #best_arm = arm\n",
        "        # Apply softmax function to the expected free energy values\n",
        "        softmax_probs = self.softmax(self.expected_free_energy)\n",
        "        self.softmax_probs = softmax_probs\n",
        "        #print(self.softmax_probs)\n",
        "\n",
        "\n",
        "        # Choose an arm probabilistically based on the softmax probabilities\n",
        "        best_arm = np.random.choice(range(self.num_arms), p=softmax_probs)\n",
        "\n",
        "        self.draws[best_arm] += 1\n",
        "\n",
        "        return best_arm\n",
        "\n",
        "    def random_agent(self):\n",
        "      choice_prob1 = round(random.random(), 3)\n",
        "      choice_prob2 = round(random.random(), 3)\n",
        "      self.softmax_probs[0] = choice_prob1 / (choice_prob1 + choice_prob2)\n",
        "      self.softmax_probs[1] = choice_prob2 / (choice_prob1 + choice_prob2)\n",
        "      best_arm = np.random.choice(range(self.num_arms), p=self.softmax_probs)\n",
        "      return best_arm\n",
        "\n",
        "    def softmax(self, vector):\n",
        "        # Calculate the softmax values\n",
        "        exp_vals = np.exp(vector)\n",
        "        probs = exp_vals / np.sum(exp_vals)\n",
        "        probs = 1-probs # because we are minimizing\n",
        "        return probs\n",
        "\n",
        "    def thompson_sampling(self):\n",
        "        max_thompson = -float('inf')\n",
        "        best_arm = 0\n",
        "        # Draw samples from the posterior distributions\n",
        "        for arm in range(self.num_arms):\n",
        "            self.thompson_mean[arm] = np.random.normal(self.posterior_mean[arm], (self.posterior_variance[arm]/np.sqrt(self.posterior_precision[arm])))\n",
        "            if self.thompson_mean[arm] >= max_thompson:\n",
        "                max_thompson = self.thompson_mean[arm]\n",
        "                best_arm = arm\n",
        "        softmax_probs = self.softmax(self.thompson_mean)\n",
        "        self.softmax_probs = softmax_probs\n",
        "\n",
        "\n",
        "        self.draws[best_arm] += 1\n",
        "\n",
        "        return best_arm\n",
        "\n",
        "\n",
        "    def ucb_action(self):\n",
        "        max_ucb = -float('inf')\n",
        "        best_arm = 0\n",
        "        for arm in range(self.num_arms):\n",
        "            # Calculate the uncertainty bonus (UCB) for each arm\n",
        "            self.ucb[arm] = self.ucb_weight * np.sqrt(1 / self.posterior_precision[arm])\n",
        "\n",
        "            # Calculate the action values (Q_t(k) + U_t(k)) for each arm\n",
        "            self.action_values[arm] = self.posterior_mean[arm] + self.ucb[arm]\n",
        "            #if self.action_values[arm] >= max_ucb:\n",
        "                # Select the arm with the highest action value\n",
        "                #max_ucb = self.action_values[arm]\n",
        "                #best_arm = arm\n",
        "        softmax_probs = self.softmax(self.action_values)\n",
        "        self.softmax_probs = softmax_probs\n",
        "        best_arm = np.random.choice(range(self.num_arms), p=softmax_probs)\n",
        "\n",
        "        self.draws[best_arm] += 1\n",
        "\n",
        "        return best_arm\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        # Update the posterior parameters for the selected arm\n",
        "        self.rewards[arm] += reward\n",
        "        self.current_reward[arm] = reward\n",
        "        self.observations[arm].append(reward)\n",
        "        self.obs_mean[arm] = np.mean(self.observations[arm])\n",
        "\n",
        "        self.posterior_precision[arm] = self.draws[arm] + self.prior_precision[arm]\n",
        "        self.posterior_mean[arm] = (self.prior_precision[arm] * self.prior_mean[arm] + self.draws[arm] * self.obs_mean[arm]) / self.posterior_precision[arm]\n",
        "        self.posterior_alpha[arm] = self.prior_alpha[arm] + 0.5 * self.draws[arm]\n",
        "        self.posterior_beta[arm] = self.prior_beta[arm] + (0.5 * np.sum((self.observations[arm]-self.obs_mean[arm])**2)) + \\\n",
        "                                    (self.prior_precision[arm] * self.draws[arm]*(self.obs_mean[arm]-self.prior_mean[arm])**2) / (2*(self.posterior_precision[arm]+self.draws[arm]))\n",
        "        self.posterior_variance[arm] = np.sqrt(np.mean(1/np.random.gamma(self.posterior_alpha[arm], 1/self.posterior_beta[arm], size = 1000))) # this is actually sd\n",
        "        self.posterior_dist_mean[arm] = mean(np.random.normal(self.posterior_mean[arm], (self.posterior_variance[arm]/np.sqrt(self.posterior_precision[arm])), size= 1000))\n",
        "\n",
        "        # update the parameters for the next iteration\n",
        "        self.prior_precision[arm] = self.posterior_precision[arm]\n",
        "        self.prior_mean[arm] = self.posterior_mean[arm]\n",
        "        self.prior_alpha[arm] = self.posterior_alpha[arm]\n",
        "        self.prior_beta[arm] = self.posterior_beta[arm]\n",
        "\n",
        "\n",
        "    def update_dynamic(self, arm, reward):\n",
        "        # Update the posterior parameters for the selected arm\n",
        "        self.rewards[arm] += reward\n",
        "        self.current_reward[arm] = reward\n",
        "        self.observations[arm].append(reward)\n",
        "        self.obs_mean[arm] = np.mean(self.observations[arm])\n",
        "\n",
        "        # weighting factor calculation\n",
        "        if len(self.observations[arm]) > 0:\n",
        "          self.weight_current[arm] = self.decay * self.weight_previous[arm] + 1\n",
        "        else:\n",
        "          self.weight_current[arm] = 1\n",
        "        self.moving_avg_current[arm] = (1 - 1/self.weight_current[arm]) * self.moving_avg_previous[arm] + (1/self.weight_current[arm]) * reward\n",
        "\n",
        "        # bayesian parameter estimation\n",
        "        self.posterior_precision[arm] = self.draws[arm] + self.prior_precision[arm]\n",
        "        self.posterior_mean[arm] = (self.prior_precision[arm] * self.prior_mean[arm] + self.draws[arm] * self.moving_avg_current[arm]) / self.posterior_precision[arm]\n",
        "        self.posterior_alpha[arm] = self.prior_alpha[arm] + 0.5 * self.draws[arm]\n",
        "        self.posterior_beta[arm] = self.prior_beta[arm] + (0.5 * np.sum((self.observations[arm]-self.moving_avg_current[arm])**2)) + \\\n",
        "                                    (self.prior_precision[arm] * self.draws[arm]*(self.moving_avg_current[arm]-self.prior_mean[arm])**2) / (2*(self.posterior_precision[arm]+self.draws[arm]))\n",
        "        self.posterior_variance[arm] = np.sqrt(np.mean(1/np.random.gamma(self.posterior_alpha[arm], 1/self.posterior_beta[arm], size = 1000))) # this is actually sd\n",
        "        self.posterior_dist_mean[arm] = mean(np.random.normal(self.posterior_mean[arm], (self.posterior_variance[arm]/np.sqrt(self.posterior_precision[arm])), size = 1000))\n",
        "\n",
        "        # update the parameters for the next iteration\n",
        "        self.prior_precision[arm] = self.posterior_precision[arm]\n",
        "        self.prior_mean[arm] = self.posterior_mean[arm]\n",
        "        self.prior_alpha[arm] = self.posterior_alpha[arm]\n",
        "        self.prior_beta[arm] = self.posterior_beta[arm]\n",
        "\n",
        "        # update weighting factor parameters for next iteration\n",
        "        self.weight_previous[arm] = self.weight_current[arm]\n",
        "        self.moving_avg_previous[arm] = self.moving_avg_current[arm]\n",
        "\n",
        "\n",
        "    def posterior_predictive(self, arm):\n",
        "        # Calculate the posterior predictive t-distribution parameters for the selected arm\n",
        "        self.t_dof[arm] = 2*(self.posterior_alpha[arm] + 1)\n",
        "        self.t_mean[arm] = self.posterior_mean[arm]\n",
        "        self.t_precision[arm] = (self.posterior_beta[arm] * (1 + self.posterior_precision[arm])) / (self.posterior_precision[arm]* (1 + self.posterior_alpha[arm]))\n",
        "        self.t_sd[arm] = 1/np.sqrt(self.t_precision[arm]) # scipy uses standard deviation, so we have to transform\n",
        "\n",
        "\n",
        "        v = self.t_dof[arm]\n",
        "        v1 = (1+v)/2\n",
        "        v2 = v/2\n",
        "        self.entropy_predictive_pdf[arm] = v1*(scipy.special.digamma(v1)-scipy.special.digamma(v2))+np.log(np.sqrt(v)*scipy.special.beta(v2,0.5))\n",
        "\n",
        "        return self.entropy_predictive_pdf[arm]\n",
        "\n",
        "    def kl_divergence(self, arm, eta):\n",
        "        entropy_of_predictive = self.posterior_predictive(arm)\n",
        "        self.kl_div[arm] = - eta*self.posterior_mean[arm] - entropy_of_predictive\n",
        "        return self.kl_div[arm]\n",
        "\n",
        "    def ambiguity(self, arm):\n",
        "        self.ambiguity_term[arm] = 0.5 * np.log(2 * np.pi * np.e) + np.log(self.posterior_beta[arm]) + scipy.special.digamma(self.posterior_alpha[arm])\n",
        "        return self.ambiguity_term[arm]\n",
        "\n",
        "    def calc_expected_free_energy(self, arm):\n",
        "        self.expected_free_energy_term = self.kl_divergence(arm, eta = self.eta) + self.ambiguity(arm)\n",
        "        return self.expected_free_energy_term\n",
        "\n",
        "    def optimal_strategy(self, true_arm_means):\n",
        "        return np.argmax(true_arm_means)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qvpz_684RJIK"
      },
      "outputs": [],
      "source": [
        "# import the data\n",
        "url = 'https://raw.githubusercontent.com/marvin-math/active_inference_multiarmed_bandit/main/musexp1.csv'\n",
        "df1 = pd.read_csv(url)\n",
        "np_array1_exp1 = df1['mu1'].to_numpy()\n",
        "np_array2_exp1 = df1['mu2'].to_numpy()\n",
        "\n",
        "url2 = 'https://raw.githubusercontent.com/marvin-math/active_inference_multiarmed_bandit/main/exp2data.csv'\n",
        "df2 = pd.read_csv(url2)\n",
        "np_array1_exp2 = df2['mu1'].to_numpy()\n",
        "np_array2_exp2 = df2['mu2'].to_numpy()\n",
        "np_arraychoice_exp2 = df2['choice'].to_numpy()\n",
        "np_arrayreward_exp2 = df2['reward'].to_numpy()\n",
        "np_arraysubject_exp2 = df2['subject'].to_numpy()\n",
        "\n",
        "url3 ='https://raw.githubusercontent.com/marvin-math/active_inference_multiarmed_bandit/main/optimization_ai.csv'\n",
        "df3 = pd.read_csv(url3)\n",
        "np_eta = df3['eta'].to_numpy()\n",
        "np_decay = df3['decay'].to_numpy()\n",
        "\n",
        "url4 ='https://raw.githubusercontent.com/marvin-math/active_inference_multiarmed_bandit/main/optimization_ucb.csv'\n",
        "df4 = pd.read_csv(url4)\n",
        "np_ucb = df4['ucb'].to_numpy()\n",
        "#np_decay = df4['decay'].to_numpy()\n",
        "\n",
        "url5 ='https://raw.githubusercontent.com/marvin-math/active_inference_multiarmed_bandit/main/df_thompson.csv'\n",
        "df5 = pd.read_csv(url5)\n",
        "#np_decay = df5['decay'].to_numpy()\n",
        "\n",
        "url6 ='https://raw.githubusercontent.com/marvin-math/active_inference_multiarmed_bandit/main/df_optimization_random.csv'\n",
        "df6 = pd.read_csv(url6)\n",
        "#np_decay = df6['decay'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srWCyviBeC1L"
      },
      "outputs": [],
      "source": [
        "from pickle import TRUE\n",
        "# initialise\n",
        "\n",
        "num_arms = 2\n",
        "\n",
        "num_total_iterations = len(np_array1_exp2)\n",
        "\n",
        "trials = np.zeros(num_total_iterations)\n",
        "expected_free_energy_arm0 = np.zeros(num_total_iterations)\n",
        "expected_free_energy_arm1 = np.zeros(num_total_iterations)\n",
        "posterior_dist_mean_arm0 = np.zeros(num_total_iterations)\n",
        "posterior_dist_mean_arm1 = np.zeros(num_total_iterations)\n",
        "posterior_variance_arm0 = np.zeros(num_total_iterations)\n",
        "posterior_variance_arm1 = np.zeros(num_total_iterations)\n",
        "posterior_mean_arm0 = np.zeros(num_total_iterations)\n",
        "posterior_mean_arm1 = np.zeros(num_total_iterations)\n",
        "posterior_precision_arm0 = np.zeros(num_total_iterations)\n",
        "posterior_precision_arm1 = np.zeros(num_total_iterations)\n",
        "posterior_alpha_arm0 = np.zeros(num_total_iterations)\n",
        "posterior_alpha_arm1 = np.zeros(num_total_iterations)\n",
        "posterior_beta_arm0 = np.zeros(num_total_iterations)\n",
        "posterior_beta_arm1 = np.zeros(num_total_iterations)\n",
        "rewards = np.zeros(num_total_iterations)\n",
        "cumulative_rewards = np.zeros(num_total_iterations)\n",
        "cumulative_optimal_rewards = np.zeros(num_total_iterations)\n",
        "thompson_mean_arm0 = np.zeros(num_total_iterations)\n",
        "thompson_mean_arm1 = np.zeros(num_total_iterations)\n",
        "ucb_arm0 = np.zeros(num_total_iterations)\n",
        "ucb_arm1 = np.zeros(num_total_iterations)\n",
        "action_values_arm0 = np.zeros(num_total_iterations)\n",
        "action_values_arm1 = np.zeros(num_total_iterations)\n",
        "chosen_arms = np.zeros(num_total_iterations)\n",
        "optimal_arms = np.zeros(num_total_iterations)\n",
        "optimal_rewards = np.zeros(num_total_iterations)\n",
        "regrets = np.zeros(num_total_iterations)\n",
        "cumulative_regrets = np.zeros(num_total_iterations)\n",
        "etas_list = np.zeros(num_total_iterations)\n",
        "decays_list = np.zeros(num_total_iterations)\n",
        "true_dist_mean0 = np.zeros(num_total_iterations)\n",
        "true_dist_mean1 = np.zeros(num_total_iterations)\n",
        "draws0 = np.zeros(num_total_iterations)\n",
        "draws1 = np.zeros(num_total_iterations)\n",
        "ucb_list = np.zeros(num_total_iterations)\n",
        "ambiguity_arm0 = np.zeros(num_total_iterations)\n",
        "ambiguity_arm1 = np.zeros(num_total_iterations)\n",
        "\n",
        "num_simulations = 1\n",
        "num_iterations_per_simulation = len(np_array1_exp2)\n",
        "\n",
        "true_arm_means_dynamic = np.empty(num_arms, dtype=object)\n",
        "true_arm_means_dynamic[:] = [[] for _ in range(num_arms)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#setup experiment environment\n",
        "environment = 'dynamic'\n",
        "experiment1 = False\n",
        "\n",
        "\n",
        "#eta = 0.3 # exploration exploitation parameter - this parameter gives the weight the posterior mean has in the decision (higher weight -> lower kl divergence -> lower expected free energy)\n",
        "#decay = 0.1 # how much are previous samples taken into account? Between 0 and 1 - higher values give more weight to older data, lower values more weight to current data\n",
        "ucb_weight = 0.6\n",
        "\n",
        "#create empty dict to store the results\n",
        "result_df = pd.DataFrame(columns=['trial', 'expected_free_energy', 'posterior_dist_mean', 'posterior_variance', 'posterior_mean', 'posterior_precision', 'posterior_alpha', 'posterior_beta', 'reward', 'cumulative_reward', 'cumulative_optimal_reward', 'thompson_mean', 'ucb', 'action_values', 'chosen_arm', 'optimal_arm', 'optimal_reward', 'regret', 'cumulative_regret', 'true_dist_mean', 'draw_sd', 'decay', 'ucb_weight', 'eta', 'draws0', 'draws1', 'ambiguity_arm0', 'ambiguity_arm1'])\n",
        "index = 0\n",
        "# Reset cumulative variables for each simulation\n",
        "cumulative_reward = 0\n",
        "cumulative_regret = 0\n",
        "cumulative_optimal_reward = 0\n",
        "\n",
        "for participant in range(44):\n",
        "    mean_of_true_rewards = 5\n",
        "    mean_of_true_sd = 3\n",
        "    prior_mean = np.ones(num_arms)\n",
        "    prior_precision = np.ones(num_arms)\n",
        "    prior_alpha = np.ones(num_arms)\n",
        "    prior_beta = np.ones(num_arms)*2\n",
        "\n",
        "    mu1 = np_array1_exp2[np_arraysubject_exp2 == (participant+1)]\n",
        "    mu2 = np_array2_exp2[np_arraysubject_exp2 == (participant+1)]\n",
        "    print(participant)\n",
        "    eta = np_eta[participant]\n",
        "    decay = np_decay[participant]\n",
        "    #ucb_weight = np_ucb[participant]\n",
        "    #eta = 0.3 # dummy\n",
        "\n",
        "\n",
        "    # Create the Bayesian bandit agent\n",
        "    agent = BayesianBanditAgent(num_arms, prior_mean, prior_precision, prior_alpha, prior_beta, eta, decay, ucb_weight)\n",
        "\n",
        "    # Simulate the multi-armed bandit problem for a certain number of iterations\n",
        "    if environment == 'stationary':\n",
        "      num_iterations = 5000\n",
        "      arm_means =  [3, 10]      #np.random.normal(mean_of_true_rewards, 1.0, num_arms)  # True mean rewards for each arm\n",
        "      arm_sds = [10, 2]      #np.random.gamma(mean_of_true_sd, 2.0, num_arms) # true sd for each arm\n",
        "\n",
        "\n",
        "    elif environment == 'dynamic':\n",
        "      if experiment1 == True:\n",
        "        true_arm_means_dynamic[0] = np_array1_exp1\n",
        "        true_arm_means_dynamic[1] = np_array2_exp1\n",
        "\n",
        "      elif experiment1 == False:\n",
        "        true_arm_means_dynamic[0] = np_array1_exp2\n",
        "        true_arm_means_dynamic[1] = np_array2_exp2\n",
        "      num_iterations = len(true_arm_means_dynamic[0])\n",
        "\n",
        "\n",
        "\n",
        "    for t in range(len(mu1)):\n",
        "      # Select an arm\n",
        "      chosen_arm = agent.select_lowest_expected_free_energy_arm()\n",
        "\n",
        "      if environment == 'stationary':\n",
        "\n",
        "        # Simulate the reward for the chosen arm (normally distributed)\n",
        "        reward = np.random.normal(arm_means[chosen_arm], arm_sds[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "\n",
        "        # Update the agent with the observed reward\n",
        "        agent.update_dynamic(chosen_arm, reward)\n",
        "      elif environment == 'dynamic':\n",
        "        true_sd = np.sqrt(10)\n",
        "        if experiment1 == True:\n",
        "          if chosen_arm == 0:\n",
        "            reward = np.random.normal(mu1[t], true_sd)\n",
        "          elif chosen_arm == 1:\n",
        "            reward = 0\n",
        "        elif experiment1 == False:\n",
        "          if chosen_arm == 0:\n",
        "            reward = np.random.normal(mu1[t], true_sd)\n",
        "          elif chosen_arm == 1:\n",
        "            reward = np.random.normal(mu2[t], true_sd)\n",
        "\n",
        "\n",
        "        cumulative_reward += reward\n",
        "        trial_number = t+1\n",
        "\n",
        "        agent.update_dynamic(chosen_arm, reward)\n",
        "        # Append values to the numpy array after each trial\n",
        "        true_arm_means = [mu1[t], mu2[t]]\n",
        "        optimal_arm = agent.optimal_strategy(true_arm_means)\n",
        "        optimal_reward = np.random.normal(true_arm_means[optimal_arm], true_sd)\n",
        "        cumulative_optimal_reward += optimal_reward\n",
        "        regret = optimal_reward - reward\n",
        "        cumulative_regret += regret\n",
        "\n",
        "        trials[index] = t + 1\n",
        "\n",
        "        # Update the values for both arms after each trial\n",
        "        chosen_arms[index] = chosen_arm\n",
        "        optimal_arms[index] = optimal_arm\n",
        "        optimal_rewards[index] = optimal_reward\n",
        "        regrets[index] = regret\n",
        "        cumulative_regrets[index] = cumulative_regret\n",
        "        etas_list[index] = eta\n",
        "        decays_list[index] = decay\n",
        "        ucb_list[index] = ucb_weight\n",
        "        expected_free_energy_arm0[index] = agent.calc_expected_free_energy(0)\n",
        "        expected_free_energy_arm1[index] = agent.calc_expected_free_energy(1)\n",
        "        posterior_dist_mean_arm0[index] = agent.posterior_dist_mean[0]\n",
        "        posterior_dist_mean_arm1[index] = agent.posterior_dist_mean[1]\n",
        "        posterior_variance_arm0[index] = agent.posterior_variance[0]\n",
        "        posterior_variance_arm1[index] = agent.posterior_variance[1]\n",
        "        posterior_mean_arm0[index] = agent.posterior_mean[0]\n",
        "        posterior_mean_arm1[index] = agent.posterior_mean[1]\n",
        "        posterior_precision_arm0[index] = agent.posterior_precision[0]\n",
        "        posterior_precision_arm1[index] = agent.posterior_precision[1]\n",
        "        posterior_alpha_arm0[index] = agent.posterior_alpha[0]\n",
        "        posterior_alpha_arm1[index] = agent.posterior_alpha[1]\n",
        "        posterior_beta_arm0[index] = agent.posterior_beta[0]\n",
        "        posterior_beta_arm1[index] = agent.posterior_beta[1]\n",
        "        rewards[index] = reward\n",
        "        cumulative_rewards[index] = cumulative_reward\n",
        "        cumulative_optimal_rewards[index] = cumulative_optimal_reward\n",
        "        thompson_mean_arm0[index] = agent.thompson_mean[0]\n",
        "        thompson_mean_arm1[index] = agent.thompson_mean[1]\n",
        "        ucb_arm0[index] = agent.ucb[0]\n",
        "        ucb_arm1[index] = agent.ucb[1]\n",
        "        action_values_arm0[index] = agent.action_values[0]\n",
        "        action_values_arm1[index] = agent.action_values[1]\n",
        "        true_dist_mean0[index] = true_arm_means[0]\n",
        "        true_dist_mean1[index] = true_arm_means[1]\n",
        "        draws0[index] = agent.draws[0]\n",
        "        draws1[index] = agent.draws[1]\n",
        "        ambiguity_arm0[index] = agent.ambiguity_term[0]\n",
        "        ambiguity_arm1[index] = agent.ambiguity_term[1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        index += 1\n",
        "\n",
        "# Create a pandas DataFrame with the collected data\n",
        "result_df = pd.DataFrame({\n",
        "    'trial': trials,\n",
        "    'eta': etas_list,\n",
        "    'decay': decays_list,\n",
        "    'ucb': ucb_list,\n",
        "    'expected_free_energy_arm0': expected_free_energy_arm0,\n",
        "    'expected_free_energy_arm1': expected_free_energy_arm1,\n",
        "    'posterior_dist_mean_arm0': posterior_dist_mean_arm0,\n",
        "    'posterior_dist_mean_arm1': posterior_dist_mean_arm1,\n",
        "    'posterior_variance_arm0': posterior_variance_arm0,\n",
        "    'posterior_variance_arm1': posterior_variance_arm1,\n",
        "    'posterior_mean_arm0': posterior_mean_arm0,\n",
        "    'posterior_mean_arm1': posterior_mean_arm1,\n",
        "    'posterior_precision_arm0': posterior_precision_arm0,\n",
        "    'posterior_precision_arm1': posterior_precision_arm1,\n",
        "    'posterior_alpha_arm0': posterior_alpha_arm0,\n",
        "    'posterior_alpha_arm1': posterior_alpha_arm1,\n",
        "    'posterior_beta_arm0': posterior_beta_arm0,\n",
        "    'posterior_beta_arm1': posterior_beta_arm1,\n",
        "    'rewards': rewards,\n",
        "    'cumulative_rewards': cumulative_rewards,\n",
        "    'cumulative_optimal_rewards': cumulative_optimal_rewards,\n",
        "    'thompson_mean_arm0': thompson_mean_arm0,\n",
        "    'thompson_mean_arm1': thompson_mean_arm1,\n",
        "    'ucb_arm0': ucb_arm0,\n",
        "    'ucb_arm1': ucb_arm1,\n",
        "    'action_values_arm0': action_values_arm0,\n",
        "    'action_values_arm1': action_values_arm1,\n",
        "    'chosen_arm': chosen_arms,\n",
        "    'optimal_arm': optimal_arms,\n",
        "    'optimal_reward': optimal_rewards,\n",
        "    'regret': regrets,\n",
        "    'cumulative_regret': cumulative_regrets,\n",
        "    'true_dist_mean0': true_dist_mean0,\n",
        "    'true_dist_mean1': true_dist_mean1,\n",
        "    'draw_sd': np.sqrt(10),\n",
        "    'draws0': draws0,\n",
        "    'draws1': draws1,\n",
        "    'ambiguity_arm0': ambiguity_arm0,\n",
        "    'ambiguity_arm1': ambiguity_arm1\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "result_df.to_csv('/content/simulation_results_ai.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Posterior Predictive PDF for New Data Point:\", posterior_predictive_pdf)\n",
        "print(f\"the posterior means are: {np.mean(agent.posterior_dist_mean)}\")\n",
        "print(f\"the posterior sd is: {np.mean(agent.posterior_variance)}\")\n",
        "#print(f\"the true arm means are: {np.mean(true_arm_means_dynamic[0])} and {np.mean(true_arm_means_dynamic[1])}\")\n",
        "print(f\"the true arm sds are: {np.sqrt(10)}\")\n",
        "print(f\"the cumulative reward is: {cumulative_reward}\")\n",
        "\n",
        "#print(f\"entropy of t-distribution is: {entropy}\")\n",
        "#print(f\"kl_divergence is: {agent.kl_divergence}\")\n",
        "#print(f\"ambiguity is: {agent.ambiguity}\")\n",
        "print(f\"the expected free energy of arm 1: {np.mean(expected_free_energy_arm0)} and arm 2: {np.mean(expected_free_energy_arm1)}\")\n",
        "#print(f\"draws: {agent.draws}\")\n",
        "#print(f\"kl divergences: {np.mean(agent.kl_div)}\")\n",
        "print(f\"total draws: {agent.draws[0] + agent.draws[1]}\")\n",
        "print(f\"the entropies of the predictives are: {np.mean(agent.entropy_predictive_pdf[0])} and {np.mean(agent.entropy_predictive_pdf[1])}\")\n",
        "print(f\"the kl divergences are: {np.mean(agent.kl_div[0])} and {np.mean(agent.kl_div[1])}\")\n",
        "print(f\"the ambiguity terms are: {np.mean(agent.ambiguity_term[0])} and {np.mean(agent.ambiguity_term[1])}\")\n",
        "\n",
        "\n",
        "for arm in range(num_arms):\n",
        "  #print(f\"mean free energy per arm: {np.mean(agent.expected_free_energies[arm])}\")\n",
        "  print(f\"arm {arm} has been chosen {int(agent.draws[arm])} times\")\n",
        "  #print(f\"observation matrix: {agent.observations}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix_V42rd1TpG"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import Bounds, minimize\n",
        "\n",
        "# initialise dataframe to save the values\n",
        "\n",
        "results_df = pd.DataFrame(columns=['Participant', 'eta', 'decay', 'Optimal Likelihood'])\n",
        "\n",
        "# this section runs the log likelihood minimzation algorithm\n",
        "num_arms = 2\n",
        "num_iterations = len(np_arrayreward_exp2)\n",
        "p = np.empty(num_arms, dtype=object)\n",
        "p[:] = [[] for _ in range(num_arms)]\n",
        "\n",
        "# Define the objective function with proper argument structure\n",
        "def neg_fe_log_lik(parameters, *args):\n",
        "    ucb_weight = 0.6\n",
        "    eta = parameters[0]\n",
        "    decay = parameters[1]\n",
        "    prior_mean = np.ones(num_arms) * 0\n",
        "    prior_precision = np.ones(num_arms)\n",
        "    prior_alpha = np.ones(num_arms)\n",
        "    prior_beta = np.ones(num_arms) * 2\n",
        "    negloglikelihood = 0\n",
        "\n",
        "    agent = BayesianBanditAgent(num_arms, prior_mean, prior_precision, prior_alpha, prior_beta, eta, decay, ucb_weight)\n",
        "\n",
        "\n",
        "\n",
        "    for t in range(len(participant_choices)):\n",
        "        #best_arm = agent.select_lowest_expected_free_energy_arm()\n",
        "        #best_arm = agent.thompson_sampling()\n",
        "        best_arm = agent.select_lowest_expected_free_energy_arm()\n",
        "        if participant_choices[t] == 0:\n",
        "            negloglikelihood += np.log(agent.softmax_probs[0]) # the posterior variance is the posterior of previour trial, so technically the prior of this trial\n",
        "        else:\n",
        "            negloglikelihood += np.log(agent.softmax_probs[1])\n",
        "        agent.update_dynamic(best_arm, participant_rewards[t])\n",
        "        #print(agent.softmax_probs)\n",
        "\n",
        "\n",
        "    negloglikelihood = (-negloglikelihood)\n",
        "    print(parameters, negloglikelihood)\n",
        "    return negloglikelihood\n",
        "\n",
        "# initial eta and decay setting\n",
        "initial_parameters = [0.7, 0.3] # 0.3 #\n",
        "\n",
        "# Define bounds for each parameter\n",
        "#ucb_bounds = (0, 2.5758)  # Example bounds for eta parameter\n",
        "decay_bounds = (0, 1)  # Example bounds for decay parameter\n",
        "eta_bounds = (0, 1)\n",
        "\n",
        "# Create a Bounds object\n",
        "parameter_bounds = Bounds([eta_bounds[0], decay_bounds[0]], [eta_bounds[1],decay_bounds[1]])\n",
        "\n",
        "# Iterate through participants and fit parameters separately\n",
        "for participant in range(44):\n",
        "    participant_choices = np_arraychoice_exp2[np_arraysubject_exp2 == (participant+1)]\n",
        "    participant_rewards = np_arrayreward_exp2[np_arraysubject_exp2 == (participant+1)]\n",
        "    print(participant)\n",
        "\n",
        "\n",
        "    result = minimize(\n",
        "        neg_fe_log_lik,\n",
        "        initial_parameters, method = 'Powell', options={'xtol':1e-3, 'ftol': 1e-3},\n",
        "        args=(participant_choices, participant_rewards),\n",
        "        bounds=parameter_bounds\n",
        "    )\n",
        "\n",
        "        # Append results to the DataFrame\n",
        "    results_df = results_df.append({\n",
        "        'Participant': participant+1,\n",
        "        'eta': result.x[0],\n",
        "        'decay': result.x[1],\n",
        "        'Optimal Likelihood': result.fun\n",
        "    }, ignore_index=True)\n",
        "\n",
        "results_df.to_csv('optimization_ai.csv', index=False)\n",
        "\n",
        "\n",
        "\"\"\"    print(\"Optimization Result for Participant\", participant + 1)\n",
        "    print(\"Optimal Parameters:\", result.x)\n",
        "    print(\"Optimal Value of the Objective Function:\", result.fun)\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2vHV7Kuea4P",
        "outputId": "39669573-c678-4c47-b460-40b5349a4eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " message: Optimization terminated successfully.\n",
            " success: True\n",
            "  status: 0\n",
            "     fun: 262.3486128551278\n",
            "       x: [ 1.246e-01]\n",
            "     nit: 3\n",
            "   direc: [[ 1.000e+00]]\n",
            "    nfev: 42\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAZjaPVPio2fqQwsBOOh7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}